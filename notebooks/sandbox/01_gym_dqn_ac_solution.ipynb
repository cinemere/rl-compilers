{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f",
      "metadata": {
        "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f"
      },
      "source": [
        "# Intro to RL: Gymnasium, DQN, Actor-critic\n",
        "\n",
        "## Part #1: OpenAI Gym / Farama Foundation Gymnasium\n",
        "\n",
        "[Gym](https://gym.openai.com) — это набор инструментов для разработки и сравнения алгоритмов обучения с подкреплением, который также включает в себя большой [набор окружений](https://gym.openai.com/envs/).\n",
        "\n",
        "[Gymnasium](https://gymnasium.farama.org/) — актуальный форк Gym'а. Имеет смысл по возможности использовать его. Но, т.к. большинство гайдов написаны под старую (pre- 0.21) версию Gym'а, то будьте готовы к [некоторым правкам кода](https://gymnasium.farama.org/content/migration-guide/). К счастью, большинство современных библиотек активно переходят или уже перешли и на Gymnasium, и на новый интерфейс."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0762ee-f55c-4bec-b544-fb96bcdb5618",
      "metadata": {
        "id": "1c0762ee-f55c-4bec-b544-fb96bcdb5618"
      },
      "outputs": [],
      "source": [
        "# Похожий код вы наверняка уже встречали — ставим нужные зависимости, если это колаб\n",
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af286eb6-aec5-4f2e-99d0-37cb25b47aa0",
      "metadata": {
        "id": "af286eb6-aec5-4f2e-99d0-37cb25b47aa0"
      },
      "source": [
        "\n",
        "\n",
        "![Mountain Car](https://gymnasium.farama.org/_images/mountain_car.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d538769-5bee-4e6a-9ab2-1115a9419074",
      "metadata": {
        "id": "5d538769-5bee-4e6a-9ab2-1115a9419074"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "np.set_printoptions(3)\n",
        "\n",
        "\n",
        "# Создаем окружение\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Инициализируем окружение\n",
        "state, info = env.reset()\n",
        "print(f\"state: {state}\")\n",
        "\n",
        "# Выполняем действие в среде\n",
        "next_state, r, terminated, truncated, info = env.step(0)\n",
        "print(f'{next_state=} | {r=} | {terminated=} | {truncated=} | {info=}')\n",
        "\n",
        "# Закрываем окружение\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc1cfdc-d029-492e-a82c-0ea6d944fed2",
      "metadata": {
        "id": "efc1cfdc-d029-492e-a82c-0ea6d944fed2"
      },
      "source": [
        "### Основные методы окружения:\n",
        "\n",
        "- `reset()` — инициализация окружения, возвращает первое наблюдение (состояние) и доп информацию.  \n",
        "- `step(a)` — выполнить в среде действие $\\mathbf{a}$ и получить кортеж: $\\mathbf{\\langle s_{t+1}, r_t, terminated, truncated, info \\rangle}$, где $\\mathbf{s_{t+1}}$ - следующее состояние, $\\mathbf{r_t}$ - вознаграждение, $\\mathbf{terminated}$ - флаг завершения эпизода, $\\mathbf{truncated}$ — флаг завершения эпизода по step-лимиту, $\\mathbf{info}$ - дополнительная информация.\n",
        "\n",
        "### Дополнительные методы:\n",
        "* `render()` — визуализация текущего состояния среды\n",
        "* `close()` — закрывает окружение\n",
        "\n",
        "\n",
        "### Свойства среды:\n",
        "* `env.observation_space` — информация о пространстве состояний\n",
        "* `env.action_space` — информация о пространстве действий\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a3060e-b4e1-4435-9c08-fc3491fa3ade",
      "metadata": {
        "id": "d9a3060e-b4e1-4435-9c08-fc3491fa3ade"
      },
      "outputs": [],
      "source": [
        "print(f'{env.observation_space=}')\n",
        "print(f'{env.action_space=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90fd4dd5-5157-4df7-bc5b-8f00304c9685",
      "metadata": {
        "id": "90fd4dd5-5157-4df7-bc5b-8f00304c9685"
      },
      "outputs": [],
      "source": [
        "# Отрисовка\n",
        "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
        "env.reset()\n",
        "print(env.render().shape)\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf91d4c-352e-449e-8f79-edc15f701892",
      "metadata": {
        "id": "3cf91d4c-352e-449e-8f79-edc15f701892"
      },
      "source": [
        "### Среда ``MountainCar-v0``\n",
        "\n",
        "Информацию о любой среде можно найти в [исходниках](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/mountain_car.py) или на [сайте](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/). О ``MountainCar-v0`` мы можем узнать следующее:\n",
        "\n",
        "#### Задание:\n",
        "Автомобиль едет по одномерному треку между двумя холмами. Цель состоит в том, чтобы заехать на правый холм; однако двигатель машины недостаточно мощный, чтобы взобраться на холм за один проход. Следовательно, единственный способ добиться успеха $-$ это двигаться вперед и назад, чтобы набрать обороты.\n",
        "\n",
        "#### Пространство состояний Box(2):\n",
        "\n",
        "\n",
        "\n",
        "Num | Observation  | Min  | Max  \n",
        "----|--------------|------|----   \n",
        "0   | position     | -1.2 | 0.6\n",
        "1   | velocity     | -0.07| 0.07\n",
        "\n",
        "\n",
        "#### Пространство действий Discrete(3):\n",
        "\n",
        "\n",
        "\n",
        "Num | Action|\n",
        "----|-------------|\n",
        "0   | push left   |\n",
        "1   | no push     |\n",
        "2   | push right  |\n",
        "\n",
        "- Вознаграждения: -1 за каждый шаг, пока не достигнута цель\n",
        "\n",
        "- Начальное состояние: Случайная позиция от -0.6 до -0.4 с нулевой скоростью."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6776d73b-2926-4ae4-8f61-d1dc37996935",
      "metadata": {
        "id": "6776d73b-2926-4ae4-8f61-d1dc37996935"
      },
      "source": [
        "### Пример со случайной стратегией:\n",
        "\n",
        "Для выбора действия используется ``env.action_space.sample()``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec5937c-e378-4080-b544-f8a4ebfd8462",
      "metadata": {
        "id": "4ec5937c-e378-4080-b544-f8a4ebfd8462"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers.record_video import RecordVideo\n",
        "\n",
        "# создаем окружение с ограничением на число шагов в среде\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\", max_episode_steps=250)\n",
        "# добавляем визуализацию\n",
        "env = RecordVideo(env, \"./video\")\n",
        "\n",
        "# проводим инициализацию и запоминаем начальное состояние\n",
        "s, _ = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # выполняем действие, получаем s, r, terminated, truncated, info\n",
        "    s, r, terminated, truncated, _ = env.step(env.action_space.sample())\n",
        "    done = terminated or truncated\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb83187-3869-46a0-8a7a-402a1b47f7b7",
      "metadata": {
        "id": "bfb83187-3869-46a0-8a7a-402a1b47f7b7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "def show_video(episode: int = 0):\n",
        "    return display(Video(f'video/rl-video-episode-{episode}.mp4', embed=True))\n",
        "\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4219ed54-2dbb-4596-9946-6edff387263d",
      "metadata": {
        "id": "4219ed54-2dbb-4596-9946-6edff387263d"
      },
      "source": [
        "### Пример с handcrafted стратегией:\n",
        "В среде MountainCar-v0 мы хотим, чтобы машина достигла флага. Давайте решим эту задачу, не используя обучение с подкреплением. Модифицируйте код функции ```act``` ниже для выполнения этого задания. Функция получает на вход состояние среды и должна вернуть действие."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b47eabe-22ca-41f5-b1da-fced9247c2f9",
      "metadata": {
        "id": "5b47eabe-22ca-41f5-b1da-fced9247c2f9"
      },
      "outputs": [],
      "source": [
        "def act(s):\n",
        "    # список возможных действий\n",
        "    left, stop, right = 0, 1, 2\n",
        "\n",
        "    # позиция и скорость\n",
        "    position, velocity = s\n",
        "    # пример: можем попробовать всегда ехать вправо\n",
        "    # action = right\n",
        "    ####### Здесь ваш код ########\n",
        "    action = left if velocity < 0 else right\n",
        "    ##############################\n",
        "    return action\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\", max_episode_steps=250)\n",
        "# добавляем визуализацию\n",
        "env = RecordVideo(env, \"./video\")\n",
        "\n",
        "# проводим инициализацию и запоминаем начальное состояние\n",
        "s, _ = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # выбор действия \"агентом\"\n",
        "    action = act(s)\n",
        "\n",
        "    # выполняем действие, получаем s, r, terminated, truncated, info\n",
        "    s, r, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "\n",
        "if s[0] > 0.47:\n",
        "    print(\"Решено!\")\n",
        "else:\n",
        "    print(\"Исправьте функцию выбора действия!\")\n",
        "\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4b03ba-6071-4050-a29f-292bbd924454",
      "metadata": {
        "id": "3e4b03ba-6071-4050-a29f-292bbd924454"
      },
      "source": [
        "## DQN\n",
        "\n",
        "В данном пункте реализуем алгоритм DQN для решения среды [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), цель которой балансировать палочкой в вертикальном положении, управляя только тележкой, к которой она прикреплена.\n",
        "\n",
        "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
        "\n",
        "![cartpole](https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24bcf642-10cc-431b-b9b2-18bc2bfbf1a3",
      "metadata": {
        "id": "24bcf642-10cc-431b-b9b2-18bc2bfbf1a3"
      },
      "source": [
        "### Action Space\n",
        "\n",
        "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
        "\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right\n",
        "\n",
        "**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
        "the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|---------------------|-------------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "**Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
        "\n",
        "- The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
        "- The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
        "   if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
        "\n",
        "### Rewards\n",
        "\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
        "including the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0.\n",
        "\n",
        "### Starting State\n",
        "\n",
        "All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
        "\n",
        "### Episode End\n",
        "\n",
        "The episode ends if any one of the following occurs:\n",
        "\n",
        "1. Termination: Pole Angle is greater than ±12°\n",
        "2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3. Truncation: Episode length is greater than 500 (200 for v0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4",
      "metadata": {
        "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "\n",
        "print(f'{env.observation_space=}')\n",
        "print(f'{env.action_space=}')\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "print(f'Action_space: {n_actions} | State_space: {env.observation_space.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381",
      "metadata": {
        "id": "febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "Будем использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``).\n",
        "\n",
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]\n",
        "$$\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22be5c6-da87-46aa-95ec-ec792e46a355",
      "metadata": {
        "id": "f22be5c6-da87-46aa-95ec-ec792e46a355"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(input_dim, ...),\n",
        "    #    torch.nn.ReLU() or Tanh(),\n",
        "    #    ...\n",
        "    # )\n",
        "    ####### Здесь ваш код ########\n",
        "    from_dim = input_dim\n",
        "    layers = []\n",
        "    for to_dim in hidden_dims:\n",
        "        layers.extend([\n",
        "            nn.Linear(from_dim, to_dim),\n",
        "            nn.Tanh()\n",
        "        ])\n",
        "        from_dim = to_dim\n",
        "\n",
        "    layers.append(nn.Linear(from_dim, output_dim))\n",
        "    network = nn.Sequential(*layers)\n",
        "    ##############################\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1684ea0-11e4-4f16-a7d3-45c97dbcbc99",
      "metadata": {
        "id": "d1684ea0-11e4-4f16-a7d3-45c97dbcbc99"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(Q, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = Q(state).detach().numpy()\n",
        "\n",
        "    # action =\n",
        "    ####### Здесь ваш код ########\n",
        "    if np.random.random() < epsilon:\n",
        "        n_actions = Q_s.shape[-1]\n",
        "        action = np.random.choice(n_actions)\n",
        "    else:\n",
        "        action = np.argmax(Q_s)\n",
        "    ##############################\n",
        "\n",
        "    action = int(action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef",
      "metadata": {
        "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef"
      },
      "outputs": [],
      "source": [
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "\n",
        "def compute_td_loss(\n",
        "        Q, states, actions, rewards, next_states, terminateds,\n",
        "        gamma=0.99, check_shapes=False, regularizer=.1\n",
        "):\n",
        "    \"\"\" Считает TD ошибку.\"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    s = to_tensor(states)                     # shape: [batch_size, state_size]\n",
        "    a = to_tensor(actions, int).long()        # shape: [batch_size]\n",
        "    r = to_tensor(rewards)                    # shape: [batch_size]\n",
        "    s_next = to_tensor(next_states)           # shape: [batch_size, state_size]\n",
        "    term = to_tensor(terminateds, bool)       # shape: [batch_size]\n",
        "\n",
        "    # получаем Q[s, a] для выбранных действий в текущих состояниях (для каждого примера из батча)\n",
        "    # Q_s_a = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    Q_s_a = torch.gather(Q(s), dim=1, index=torch.unsqueeze(a, 1)).squeeze(-1)\n",
        "    ##############################\n",
        "\n",
        "    # получаем Q[s_next, *] — значения полезности всех действий в следующих состояниях\n",
        "    # Q_sn = ...,\n",
        "    # а затем вычисляем V*[s_next] — оптимальные значения полезности следующих состояний\n",
        "    # V_sn = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    with torch.no_grad():\n",
        "        Q_sn = Q(s_next)\n",
        "        V_sn, _ = torch.max(Q_sn, axis=-1)\n",
        "    ##############################\n",
        "\n",
        "    assert V_sn.dtype == torch.float32\n",
        "\n",
        "    # вычисляем TD target и далее TD error\n",
        "    # target = ...\n",
        "    # td_error = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    target = r + gamma * V_sn * torch.logical_not(term)\n",
        "    td_error = Q_s_a - target\n",
        "    ##############################\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    loss = torch.mean(td_error ** 2)\n",
        "    # добавляем регуляризацию на значения Q\n",
        "    loss += regularizer * Q_s_a.mean()\n",
        "\n",
        "    if check_shapes:\n",
        "        assert Q_sn.data.dim(\n",
        "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert V_sn.data.dim(\n",
        "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target.data.dim(\n",
        "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d",
      "metadata": {
        "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d"
      },
      "outputs": [],
      "source": [
        "def eval_dqn(env_name, Q):\n",
        "    \"\"\"Оценка качества работы алгоритма на одном эпизоде\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    s, _ = env.reset()\n",
        "    done, ep_return = False, 0.\n",
        "\n",
        "    while not done:\n",
        "        # set epsilon = 0 to make an agent act greedy\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=0.)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        ep_return += r\n",
        "        s = s_next\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return ep_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2be9060-bba6-4d9a-9e4b-0aa50f88839e",
      "metadata": {
        "id": "d2be9060-bba6-4d9a-9e4b-0aa50f88839e"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "def linear(st, end, duration, t):\n",
        "    if t >= duration:\n",
        "        return end\n",
        "    return st + (end - st) * (t / duration)\n",
        "\n",
        "def run_dqn(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(128, 128), lr=1e-3,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.5, total_max_steps=100_000,\n",
        "        train_schedule=1, eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps+1):\n",
        "        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            opt.zero_grad()\n",
        "            loss = compute_td_loss(Q, [s], [a], [r], [s_next], [terminated])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be1c2a8-37c6-413d-9d57-eff7251943ab",
      "metadata": {
        "id": "6be1c2a8-37c6-413d-9d57-eff7251943ab"
      },
      "source": [
        "### DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', 1_\\text{terminated})\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b5af1a-cd63-4642-9ec2-f1dc71215dff",
      "metadata": {
        "id": "a7b5af1a-cd63-4642-9ec2-f1dc71215dff"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, terminateds\n",
        "    ####### Здесь ваш код ########\n",
        "    n_samples = min(len(replay_buffer), n_samples)\n",
        "\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples, replace=False)\n",
        "    states, actions, rewards, next_actions, terminateds = [], [], [], [], []\n",
        "    for i in indices:\n",
        "        s, a, r, n_s, done = replay_buffer[i]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        next_actions.append(n_s)\n",
        "        terminateds.append(done)\n",
        "    ##############################\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(terminateds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a8db42-6e80-4a38-8449-f13c50ef849c",
      "metadata": {
        "id": "76a8db42-6e80-4a38-8449-f13c50ef849c"
      },
      "outputs": [],
      "source": [
        "def run_dqn_rb(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(256, 256), lr=2e-3,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.5, total_max_steps=200_000,\n",
        "        train_schedule=4, replay_buffer_size=400, batch_size=32,\n",
        "        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps+1):\n",
        "        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        replay_buffer.append((s, a, r, s_next, terminated))\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            train_batch = sample_batch(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, terminateds = train_batch\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss = compute_td_loss(Q, states, actions, rewards, next_states, terminateds)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn_rb()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad8e4ac-75ce-48bf-8a33-8df95e6234a3",
      "metadata": {
        "id": "3ad8e4ac-75ce-48bf-8a33-8df95e6234a3"
      },
      "source": [
        "## Actor-Critic\n",
        "\n",
        "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
        "\n",
        "Встает вопрос, как оценить $Q^\\pi(s, a)$? В чистом policy-based алгоритме REINFORCE используется отдача $R_t$, полученная методом Монте-Карло в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции — критика.\n",
        "\n",
        "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое [семейство](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) различных алгоритмов.\n",
        "\n",
        "Сейчас познакомимся с наиболее простым вариантом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671f300d-f4fe-4396-84a7-6abe1e6ef466",
      "metadata": {
        "id": "671f300d-f4fe-4396-84a7-6abe1e6ef466"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class ActorBatch:\n",
        "    def __init__(self):\n",
        "        self.logprobs = []\n",
        "        self.q_values = []\n",
        "\n",
        "    def append(self, log_prob, q_value):\n",
        "        self.logprobs.append(log_prob)\n",
        "        self.q_values.append(q_value)\n",
        "\n",
        "    def clear(self):\n",
        "        self.logprobs.clear()\n",
        "        self.q_values.clear()\n",
        "\n",
        "\n",
        "class ActorCriticModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Инициализируйте сеть агента с двумя головами: softmax-актора и линейного критика\n",
        "        # self.net, self.actor_head, self.critic_head =\n",
        "        ####### Здесь ваш код ########\n",
        "        from_dim = input_dim\n",
        "        layers = []\n",
        "        for to_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(from_dim, to_dim),\n",
        "                nn.Tanh()\n",
        "            ])\n",
        "            from_dim = to_dim\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], output_dim),\n",
        "        )\n",
        "        # self.actor_head = nn.Sequential(\n",
        "        #     create_network(input_dim, hidden_dims, output_dim),\n",
        "        #     nn.Softmax(dim=-1)\n",
        "        # )\n",
        "        # self.critic_head = create_network(input_dim, hidden_dims, output_dim)\n",
        "        ##############################\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
        "        ####### Здесь ваш код ########\n",
        "        state = self.net(state)\n",
        "\n",
        "        action_probs = self.actor_head(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        q_values = self.critic_head(state)\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        action = action.item()\n",
        "        # adv = q_values[action] - torch.sum(q_values * action_probs, -1)\n",
        "        q_value = q_values[action]\n",
        "        ##############################\n",
        "\n",
        "        return action, log_prob, q_value\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        # Вычислите значения Q-функции для данного состояния\n",
        "        ####### Здесь ваш код ########\n",
        "        state = self.net(state)\n",
        "\n",
        "        q_values = self.critic_head(state)\n",
        "        ##############################\n",
        "        return q_values\n",
        "\n",
        "\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, hidden_dims, lr, gamma, critic_rb_size):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
        "        ####### Здесь ваш код ########\n",
        "        self.actor_critic = ActorCriticModel(state_dim, hidden_dims, action_dim)\n",
        "        self.actor_opt = self.critic_opt = self.opt = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
        "        # self.actor_opt = torch.optim.Adam(self.actor_critic.actor_head.parameters(), lr=lr)\n",
        "        # self.critic_opt = torch.optim.Adam(self.actor_critic.critic_head.parameters(), lr=lr*.25)\n",
        "        ##############################\n",
        "\n",
        "        self.actor_batch = ActorBatch()\n",
        "        self.critic_rb = deque(maxlen=critic_rb_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
        "        # Не забудьте сделать q_value.detach()\n",
        "        # self.actor_batch.append(..)\n",
        "        ####### Здесь ваш код ########\n",
        "        action, logprob, q_value = self.actor_critic(to_tensor(state))\n",
        "        self.actor_batch.append(logprob, q_value.detach())\n",
        "\n",
        "        # action = select_action_eps_greedy(self.actor_critic.critic_head, state, 0.05)\n",
        "        ##############################\n",
        "\n",
        "        return action\n",
        "\n",
        "    def append_to_replay_buffer(self, s, a, r, next_s, terminated):\n",
        "        # Добавьте новый экземпляр данных в память прецедентов.\n",
        "        ####### Здесь ваш код ########\n",
        "        self.critic_rb.append((s, a, r, next_s, terminated))\n",
        "        ##############################\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        return self.actor_critic.evaluate(state)\n",
        "\n",
        "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "        if len(self.actor_batch.q_values) < rollout_size:\n",
        "            return\n",
        "\n",
        "        self.update_actor()\n",
        "        self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
        "\n",
        "    def update_actor(self):\n",
        "        Q_s_a = to_tensor(self.actor_batch.q_values)\n",
        "        logprobs = torch.stack(self.actor_batch.logprobs)\n",
        "\n",
        "        # Реализуйте шаг обновления актора — вычислите ошибку `loss` и произведите шаг обновления градиентным спуском.\n",
        "        # Опционально: сделайте нормализацию отдач\n",
        "        ####### Здесь ваш код ########\n",
        "        # Нормализация отдач\n",
        "        Q_s_a = (Q_s_a - Q_s_a.mean()) / (Q_s_a.std() + 1e-7)\n",
        "\n",
        "        # Считаем ошибку\n",
        "        loss = torch.mean(-logprobs * Q_s_a)\n",
        "\n",
        "        self.actor_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_opt.step()\n",
        "        self.actor_batch.clear()\n",
        "        ##############################\n",
        "\n",
        "    def update_critic(self, batch_size, n_updates=1):\n",
        "        # Реализуйте n_updates шагов обучения критика.\n",
        "        ####### Здесь ваш код ########\n",
        "\n",
        "        if len(self.critic_rb) < batch_size:\n",
        "            return\n",
        "\n",
        "        for _ in range(n_updates):\n",
        "            train_batch = sample_batch(self.critic_rb, batch_size)\n",
        "            states, actions, rewards, next_states, terminateds = train_batch\n",
        "\n",
        "            self.critic_opt.zero_grad()\n",
        "            loss = self.compute_td_loss(states, actions, rewards, next_states, terminateds)\n",
        "            loss.backward()\n",
        "            self.critic_opt.step()\n",
        "        ##############################\n",
        "\n",
        "    def compute_td_loss(\n",
        "        self, states, actions, rewards, next_states, terminateds, regularizer=.1\n",
        "    ):\n",
        "        # переводим входные данные в тензоры\n",
        "        s = to_tensor(states)                     # shape: [batch_size, state_size]\n",
        "        a = to_tensor(actions, int).long()        # shape: [batch_size]\n",
        "        r = to_tensor(rewards)                    # shape: [batch_size]\n",
        "        s_next = to_tensor(next_states)           # shape: [batch_size, state_size]\n",
        "        term = to_tensor(terminateds, bool)       # shape: [batch_size]\n",
        "\n",
        "\n",
        "        # получаем Q[s, a] для выбранных действий в текущих состояниях (для каждого примера из батча)\n",
        "        # Q_s_a = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        Q_s_a = torch.gather(\n",
        "            self.evaluate(s), dim=1, index=torch.unsqueeze(a, 1)\n",
        "        ).squeeze(1)\n",
        "        ##############################\n",
        "\n",
        "        # получаем Q[s_next, *] — значения полезности всех действий в следующих состояниях\n",
        "        # Q_sn = ...,\n",
        "        # а затем вычисляем V*[s_next] — оптимальные значения полезности следующих состояний\n",
        "        # V_sn = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        with torch.no_grad():\n",
        "            Q_sn = self.evaluate(s_next)\n",
        "            V_sn, _ = torch.max(Q_sn, axis=-1)\n",
        "        ##############################\n",
        "\n",
        "        # вычисляем TD target и далее TD error\n",
        "        # target = ...\n",
        "        # td_error = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        target = r + self.gamma * V_sn * torch.logical_not(term)\n",
        "        td_error = Q_s_a - target\n",
        "        ##############################\n",
        "\n",
        "        # MSE loss для минимизации\n",
        "        loss = torch.mean(td_error ** 2)\n",
        "        # добавляем регуляризацию на значения Q\n",
        "        loss += regularizer * Q_s_a.mean()\n",
        "        return loss\n",
        "\n",
        "def run_actor_critic(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(128, 128), lr=1e-3,\n",
        "        total_max_steps=200_000,\n",
        "        # train_schedule=4, replay_buffer_size=4, batch_size=4, critic_updates_per_actor=1,\n",
        "        train_schedule=32, replay_buffer_size=5000, batch_size=32, critic_updates_per_actor=4,\n",
        "        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    episode_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    agent = ActorCriticAgent(\n",
        "        state_dim=env.observation_space.shape[0], action_dim=env.action_space.n, hidden_dims=hidden_dims,\n",
        "        lr=lr, gamma=.995, critic_rb_size=replay_buffer_size\n",
        "    )\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done, episode_return = False, 0.\n",
        "    eval = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps+1):\n",
        "        a = agent.act(s)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        episode_return += r\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # train step\n",
        "        agent.append_to_replay_buffer(s, a, r, s_next, terminated)\n",
        "        agent.update(train_schedule, batch_size, critic_updates_per_actor)\n",
        "\n",
        "        # evaluate\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval = True\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            if eval:\n",
        "                episode_return_history.append(episode_return)\n",
        "                avg_return = np.mean(episode_return_history)\n",
        "                print(f'{global_step=} | {avg_return=:.3f}')\n",
        "                if avg_return >= success_ret:\n",
        "                    print('Решено!')\n",
        "                    break\n",
        "\n",
        "            s, _ = env.reset()\n",
        "            done, episode_return = False, 0.\n",
        "            eval = False\n",
        "\n",
        "run_actor_critic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac56ab88-86bc-4509-8d3f-8328263dbd93",
      "metadata": {
        "id": "ac56ab88-86bc-4509-8d3f-8328263dbd93"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}